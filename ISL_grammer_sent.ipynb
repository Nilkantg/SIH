{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CCnnjeiQ1I5Z",
    "outputId": "df1c4f63-3e8e-4147-f728-82d29be36f61"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-iibIp7rK8S"
   },
   "source": [
    "**SUBJECT - VERB - OBJECT -------> SUBJECT - OBJECT - VERB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Te5XHz7G1K8P",
    "outputId": "987c1468-a678-4e26-9802-3b459521b4ff"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the sentence that want to convert into ISL:  I want to be a Data Scientist\n"
     ]
    }
   ],
   "source": [
    "user_input = str(input(\"Enter the sentence that want to convert into ISL: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VCJeti9W1K_P"
   },
   "outputs": [],
   "source": [
    "stopwords_isl = [\n",
    "    # Articles\n",
    "    \"a\", \"an\", \"the\",\n",
    "\n",
    "    # Prepositions\n",
    "    \"at\", \"in\", \"on\", \"by\", \"with\", \"to\", \"from\", \"of\", \"for\", \"about\", \"into\", \"onto\",\n",
    "\n",
    "    # Auxiliary/Helping Verbs\n",
    "    \"is\", \"am\", \"are\", \"was\", \"were\", \"be\", \"being\", \"been\",\n",
    "    \"do\", \"does\", \"did\", \"have\", \"has\", \"had\",\n",
    "    \"will\", \"would\", \"shall\", \"should\", \"can\", \"could\", \"may\", \"might\", \"must\",\n",
    "\n",
    "    # Conjunctions\n",
    "    \"and\", \"but\", \"or\", \"so\", \"yet\", \"while\", \"because\", \"although\", \"since\", \"if\", \"though\",\n",
    "\n",
    "    # Pronouns\n",
    "    \"it\", \"this\", \"that\", \"these\", \"those\",\n",
    "    \"which\", \"who\", \"whom\", \"whose\",\n",
    "\n",
    "    # Adverbs of Degree\n",
    "    \"very\", \"too\", \"so\", \"just\", \"only\", \"much\", \"more\", \"less\",\n",
    "    \"almost\", \"nearly\", \"hardly\", \"barely\",\n",
    "\n",
    "    # Miscellaneous\n",
    "    \"not\", \"no\", \"yes\", \"some\", \"any\", \"every\", \"many\", \"few\",\n",
    "    \"several\", \"here\", \"there\", \"now\", \"then\", \"also\", \"even\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQhqbHh56qK0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Ng7Kq5Ns4i-u"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def preprocessing_text(text):\n",
    "\n",
    "  text = text.lower()\n",
    "\n",
    "  cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "  cleaned_text = [word for word in cleaned_text.split() if word not in stopwords_isl ]\n",
    "\n",
    "  lemmatized_words = [lemmatizer.lemmatize(word) for word in cleaned_text]\n",
    "\n",
    "  lemmatized_sentence = ' '.join(lemmatized_words)\n",
    "\n",
    "  return lemmatized_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pvX_Svtv4jDc",
    "outputId": "fc8f3efc-e31c-4c2b-91ff-41d2957ea039"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i want data scientist'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text = preprocessing_text(user_input)\n",
    "cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Wx-IrzpsmvV_"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4Kq-15c4jF8"
   },
   "outputs": [],
   "source": [
    "# doc = nlp(cleaned_text)\n",
    "\n",
    "# for token in doc:\n",
    "#   print(token.text, token.pos_, token.tag_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Dro8W6HcmU9k"
   },
   "outputs": [],
   "source": [
    "def convert_to_ISL(doc):\n",
    "\n",
    "  object_items = {}\n",
    "\n",
    "  subject = []\n",
    "  verb = []\n",
    "\n",
    "  cleaned_doc = preprocessing_text(doc)\n",
    "  doc = nlp(cleaned_doc)\n",
    "\n",
    "  for token in doc:\n",
    "    if token.dep_ == \"nsubj\":\n",
    "      subject.append(token.text)\n",
    "    elif (token.dep_ in (\"dobj\", \"dative\", \"iobj\", \"pobj\")) or (token.pos_ in (\"NOUN\", \"PROPN\", \"PRON\")):\n",
    "      object_items[token.text] = token.pos_\n",
    "    elif (token.pos_ == \"VERB\") or (token.dep_ in (\"ROOT\", \"xcomp\")) :\n",
    "      verb.append(token.text)\n",
    "\n",
    "  noun_items = [k for k, v in object_items.items() if v == 'NOUN']\n",
    "  other_items = [k for k, v in object_items.items() if v != 'NOUN']\n",
    "\n",
    "  object = noun_items + other_items\n",
    "\n",
    "  text = \" \".join(subject + object + verb)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "Xmu5nBUXmVAJ",
    "outputId": "1755ea7a-972c-432c-e633-969a074c099b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON nsubj PRP\n",
      "love VERB ROOT VBP\n",
      "Jacqueline PROPN compound NNP\n",
      "Fernandez PROPN dobj NNP\n"
     ]
    }
   ],
   "source": [
    "t1 = \"The boy threw the frisbee\"\n",
    "t2 = \"He gave her a gift\"\n",
    "t3 = \"They sent their teacher a thank you note\"\n",
    "t4 = \"John sent Mary a letter\"\n",
    "t5 = \"I want to become data scientist\"\n",
    "t6 = \"I love Jacqueline Fernandez\"\n",
    "for token in nlp(t6):\n",
    "  print(token.text, token.pos_, token.dep_, token.tag_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "PEt4GLtnewjf",
    "outputId": "81e77a87-c79d-4f0b-fb0d-e5a2f407757c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i jacqueline fernandez love'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_ISL(t6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "ggiUkssSy3mn",
    "outputId": "ff7de6a4-4c24-445a-8935-85a232d04a76"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'boy threw frisbee'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_text(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "adPEdlVdmVIG"
   },
   "outputs": [],
   "source": [
    "ISL_text = convert_to_ISL(t6)\n",
    "\n",
    "with open(\"words.txt\", \"r\") as words_file:\n",
    "    words_set = set(word.strip() for word in words_file)\n",
    "    \n",
    "sentence = []\n",
    "for word in ISL_text.split():\n",
    "    if word in words_set:\n",
    "        sentence.append(word)\n",
    "    else:\n",
    "        for alphabet in word:\n",
    "            sentence.append(alphabet)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "S8VEC05g4jJG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'j', 'a', 'c', 'q', 'u', 'e', 'l', 'i', 'n', 'e', 'f', 'e', 'r', 'n', 'a', 'n', 'd', 'e', 'z', 'love']\n"
     ]
    }
   ],
   "source": [
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLSKmcV71LB3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJHXfh-M1LH0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
